{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202: Assignment 2B\n",
    "## Ayush Sharma\n",
    "## 30823293\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import udf\n",
    "import datetime \n",
    "import time\n",
    "from pyspark.ml import PipelineModel\n",
    "import datetime as dt\n",
    "from  pyspark.sql.functions import abs\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[2]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Spark Streaming Data\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name).set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "Ingest the streaming data into Spark Streaming for both process and memory activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify topic\n",
    "topic = \"NewLinuxMemoryStream2B\"\n",
    "#loading streaming data\n",
    "memoryDf = spark \\\n",
    "           .readStream \\\n",
    "           .format(\"kafka\") \\\n",
    "           .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "           .option(\"subscribe\", topic) \\\n",
    "           .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify topic\n",
    "topic = \"NewLinuxProcessStream2B\"\n",
    "\n",
    "\n",
    "processDf = spark \\\n",
    "           .readStream \\\n",
    "           .format(\"kafka\") \\\n",
    "           .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "           .option(\"subscribe\", topic) \\\n",
    "           .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryDf = memoryDf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processDf = processDf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for Memory Stream\n",
    "memorySchema = ArrayType(StructType([    \n",
    "  StructField(\"sequence\", IntegerType(), True),\n",
    "  StructField(\"machine\", IntegerType(), True),\n",
    "  StructField(\"PID\", IntegerType(), True),\n",
    "  StructField(\"MINFLT\", StringType(), True),\n",
    "  StructField(\"MAJFLT\", StringType(), True),\n",
    "  StructField(\"VSTEXT\", StringType(), True),\n",
    "  StructField(\"VSIZE\", FloatType(), True),\n",
    "  StructField(\"RSIZE\", StringType(), True),\n",
    "  StructField(\"VGROW\", StringType(), True),\n",
    "  StructField(\"RGROW\", StringType(), True),\n",
    "  StructField(\"MEM\", FloatType(), True),\n",
    "  StructField(\"CMD\", StringType(), True),\n",
    "  StructField(\"ts\", IntegerType(), True)   \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for Process Stream\n",
    "\n",
    "processSchema = ArrayType(StructType([\n",
    "    StructField(\"sequence\", IntegerType(), True),\n",
    "    StructField(\"machine\", IntegerType(), True),\n",
    "    StructField(\"PID\", IntegerType(), True),\n",
    "    StructField(\"TRUN\", IntegerType(), True),\n",
    "    StructField(\"TSLPI\", IntegerType(), True),\n",
    "    StructField(\"TSLPU\", IntegerType(), True),\n",
    "    StructField(\"POLI\", StringType(), True),\n",
    "    StructField(\"NICE\", FloatType(), True),\n",
    "    StructField(\"PRI\", FloatType(), True),\n",
    "    StructField(\"RTPR\", FloatType(), True),\n",
    "    StructField(\"CPUNR\", FloatType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"EXC\", IntegerType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"CPU\", FloatType(), True),\n",
    "    StructField(\"CMD\", StringType(), True),\n",
    "    StructField(\"ts\", IntegerType(), True),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryDf = memoryDf.select(F.from_json(F.col(\"value\").cast(\"string\"),memorySchema)\\\n",
    "                           .alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processDf = processDf.select(F.from_json(F.col(\"value\").cast(\"string\"),processSchema)\\\n",
    "                             .alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryDf = memoryDf.select(F.explode(F.col(\"parsed_value\"))\\\n",
    "                           .alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processDf = processDf.select(F.explode(F.col(\"parsed_value\"))\\\n",
    "                             .alias('unnested_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New formated schema for memory\n",
    "newMemoryDf = memoryDf.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"Sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"Machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.MINFLT\").alias(\"MINFLT\"),\n",
    "                    F.col(\"unnested_value.MAJFLT\").alias(\"MAJFLT\"),\n",
    "                    F.col(\"unnested_value.VSTEXT\").alias(\"VSTEXT\"),\n",
    "                    F.col(\"unnested_value.VSIZE\").alias(\"VSIZE\"),\n",
    "                    F.col(\"unnested_value.RSIZE\").alias(\"RSIZE\"),\n",
    "                    F.col(\"unnested_value.VGROW\").alias(\"VGROW\"),\n",
    "                    F.col(\"unnested_value.RGROW\").alias(\"RGROW\"),\n",
    "                    F.col(\"unnested_value.MEM\").alias(\"MEM\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")                    \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New formated schema for memory\n",
    "newProcessDf = processDf.select(\n",
    "                    F.col(\"unnested_value.sequence\").alias(\"Sequence\"),\n",
    "                    F.col(\"unnested_value.machine\").alias(\"Machine\"),\n",
    "                    F.col(\"unnested_value.PID\").alias(\"PID\"),\n",
    "                    F.col(\"unnested_value.TRUN\").alias(\"TRUN\"),\n",
    "                    F.col(\"unnested_value.TSLPI\").alias(\"TSLPI\"),\n",
    "                    F.col(\"unnested_value.TSLPU\").alias(\"TSLPU\"),\n",
    "                    F.col(\"unnested_value.POLI\").alias(\"POLI\"),\n",
    "                    F.col(\"unnested_value.NICE\").alias(\"NICE\"),\n",
    "                    F.col(\"unnested_value.PRI\").alias(\"PRI\"),\n",
    "                    F.col(\"unnested_value.RTPR\").alias(\"RTPR\"),\n",
    "                    F.col(\"unnested_value.CPUNR\").alias(\"CPUNR\"),\n",
    "                    F.col(\"unnested_value.Status\").alias(\"Status\"),\n",
    "                    F.col(\"unnested_value.EXC\").alias(\"EXC\"),\n",
    "                    F.col(\"unnested_value.State\").alias(\"State\"),\n",
    "                    F.col(\"unnested_value.CPU\").alias(\"CPU\"),\n",
    "                    F.col(\"unnested_value.CMD\").alias(\"CMD\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "\n",
    "Reformating the Streaming Data from memory and process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#MINFLT replace and type change\n",
    "replaceK = udf(lambda x: float(x.replace(\"K\", \"\"))*1000 if \"K\" in x else float(x), FloatType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"MINFLT\", replaceK(\"MINFLT\"))\n",
    "\n",
    "#MAJFLT replace and type change\n",
    "replaceM = udf(lambda x: float(x.replace(\"M\", \"\"))*1000000 if \"M\" in x else float(x), FloatType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"MAJFLT\", replaceM(\"MAJFLT\"))\n",
    "\n",
    "#VSTEXT replace and type change\n",
    "replaceK = udf(lambda x: float(x.replace(\"K\", \"\"))*1000 if \"K\" in x else float(x), FloatType())\n",
    "newMemoryDf= newMemoryDf.withColumn(\"VSTEXT\", replaceK(\"VSTEXT\"))\n",
    "\n",
    "\n",
    "#RSIZE replace and type change\n",
    "replaceM = udf(lambda x: str(float(x.replace(\"M\", \"\"))*1000000) if \"M\" in x else x, StringType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"RSIZE\", replaceM(\"RSIZE\"))\n",
    "\n",
    "replaceK = udf(lambda y: str(float(y.replace(\"K\", \"\"))*1000) if \"K\" in y else y, StringType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"RSIZE\", replaceK(\"RSIZE\"))\n",
    "\n",
    "newMemoryDf = newMemoryDf.withColumn(\"RSIZE\", newMemoryDf[\"RSIZE\"].cast(FloatType()))\n",
    "\n",
    "#VGROW replace and type change\n",
    "replaceSPC = udf(lambda x: str(float(x.replace(\" \", \"\"))) if \" \" in x else x, StringType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"VGROW\", replaceSPC(\"VGROW\"))\n",
    "\n",
    "replaceK = udf(lambda y: str(float(y.replace(\"K\", \"\"))*1000) if \"K\" in y else y, StringType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"VGROW\", replaceK(\"VGROW\"))\n",
    "\n",
    "newMemoryDf = newMemoryDf.withColumn(\"VGROW\", newMemoryDf[\"VGROW\"].cast(FloatType()))\n",
    "\n",
    "#RGROW replace and type change\n",
    "replaceK = udf(lambda x: float(x.replace(\"K\", \"\"))*1000 if \"K\" in x else float(x), FloatType())\n",
    "newMemoryDf = newMemoryDf.withColumn(\"RGROW\", replaceK(\"RGROW\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UDF for mapping PRI and NICE\n",
    "newPRI_Nice = udf(lambda x: x - 120  , FloatType())\n",
    "newProcessDf = newProcessDf.withColumn(\"NICE\", newPRI_Nice('PRI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.4\n",
    "\n",
    "Introducting new column by merging CMD and PID and a new event_time using ts column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for New column CMD_PRI from CMD and PRI for process\n",
    "newCMD_PID = udf(lambda x: str(x[0] + \"_\" + x[1]), StringType())\n",
    "newProcessDf = newProcessDf.withColumn(\"CMD_PID\", newCMD_PID(array('CMD', 'PID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for New column CMD_PRI from CMD and PRI for memory\n",
    "newMemoryDf = newMemoryDf.withColumn(\"CMD_PID\", newCMD_PID(array('CMD', 'PID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding event_time using the ts columnn\n",
    "eventTime = udf(lambda x: datetime.datetime.strptime(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)),'%Y-%m-%d %H:%M:%S'),TimestampType())\n",
    "# For process\n",
    "newProcessDf = newProcessDf.withColumn(\"event_time\", eventTime('ts'))\n",
    "# For memory\n",
    "newMemoryDf = newMemoryDf.withColumn(\"event_time\", eventTime('ts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and writing into parquet files for memory stream\n",
    "query_fileSink_process = newProcessDf.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the file_sink query\n",
    "query_fileSink_process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and writing into parquet files for process stream\n",
    "query_fileSink_memory = newMemoryDf.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"memory.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"memory.parquet/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the file_sink query\n",
    "query_fileSink_memory.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load process pipeline \n",
    "process_pipelineModel = PipelineModel.load('process_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading memory pipeline \n",
    "memory_pipelineModel=PipelineModel.load('memory_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Process Model\n",
    "Process_predict=process_pipelineModel.transform(newProcessDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Process Model\n",
    "memory_predict = memory_pipelineModel.transform(newMemoryDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.7 A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtered based on if it is attacked i.e. =1 for process\n",
    "process_predictAttack = Process_predict.filter(Process_predict.prediction == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data filtered based on if it is attacked i.e. =1 for memory\n",
    "memory_predictAttack = memory_predict.filter(memory_predict.prediction == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating event group by based on event_time\n",
    "windowedCounts = process_predictAttack \\\n",
    "    .withWatermark(\"event_time\", \"120 seconds\") \\\n",
    "    .groupBy(window(process_predictAttack.event_time, \"120 seconds\"),process_predictAttack.Machine)\\\n",
    "    .agg(F.count(\"*\").alias(\"total_Attack\"))\\\n",
    "    .select(\"window\",\"total_Attack\",\"Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running and streaming query\n",
    "processAttackquery = windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"queryProcessPrediction\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the query\n",
    "processAttackquery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for attack based on event_time and Machine\n",
    "windowedCounts1 = memory_predictAttack \\\n",
    "    .withWatermark(\"event_time\", \"120 seconds\") \\\n",
    "    .groupBy(window(memory_predictAttack.event_time, \"120 seconds\"),memory_predictAttack.Machine)\\\n",
    "    .agg(F.count(\"*\").alias(\"total_attack\"))\\\n",
    "    .select(\"window\",\"total_attack\",\"Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "processAttackqueryNew = windowedCounts1 \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"queryMemoryPrediction\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping the query\n",
    "processAttackqueryNew.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.7 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating column names in memory\n",
    "newMemoryPrediction = memory_predictAttack.withColumnRenamed(\"ts\",\"ts_memory\").withColumnRenamed(\"CMD_PID\",\"CMD_PID_memory\")\\\n",
    "                    .withColumnRenamed(\"prediction\",\"prediction_memory\").withColumnRenamed(\"machine\",\"machine_memory\").withColumnRenamed(\"event_time\",\"event_time_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating column names in memory\n",
    "newProcessPrediction = process_predictAttack.withColumnRenamed(\"ts\",\"ts_process\").withColumnRenamed(\"CMD_PID\",\"CMD_PID_process\")\\\n",
    "                    .withColumnRenamed(\"prediction\",\"prediction_process\").withColumnRenamed(\"machine\",\"machine_process\").withColumnRenamed(\"event_time\",\"event_time_process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the process and memory dataframe\n",
    "newFinalData = newMemoryPrediction.join(newProcessPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filterring for event times in process and memory is less than 30 seconds\n",
    "# filtering memory and process abased on event_time\n",
    "newFinalData1 = newFinalData.filter(abs(newFinalData.ts_process - newFinalData.ts_memory) <= 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column Processing_Time\n",
    "newFinalData2 = newFinalData1.withColumn(\"processing_time\",lit(dt.datetime.utcnow()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sink_merged = newFinalData2.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"process_memory_attack.parquet\")\\\n",
    "        .option(\"checkpointLocation\", \"process_memory_attack/checkpoint\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sink_merged.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
